# Containerization 

When installing software, you may come across applications that have complex chains of dependencies that are challenging to compile and install. Some software may require very specific versions of libraries that may not be available on CCR's systems or conflict with libraries needed for other applications. You may also need to move between several workstations or HPC platforms, which often requires reinstalling your software on each system. Containers are a good way to tackle all of these issues and more.

## Containerization Fundamentals

Containers build upon an idea that has long existed within computing: hardware can be emulated through software. **Virtualization** simulates some or all components of computation through a software application. Virtual machines use this concept to generate an entire operating system as an application on a host system. Containers follow the same idea, but at a much smaller scale and contained within a system's kernel.

**Containers** are portable compartmentalizations of an operating system, software, libraries, data, and/or workflows. Containers offer portability and reproducibility. 

- Portability: containers can run on any system equipped with its specified container manager.
- Reproducibility: because containers are instances of prebuilt isolated software, software will always execute the same every time.

Containers distinguish themselves through their low computational overhead and their ability to utilize all of a host system’s resources. Building containers is a relatively simple process that starts with a container engine.

## Container engines

[Docker](https://www.docker.com/) is the most widely used container engine, and  can be used on any system where you have administrative privileges. _Docker cannot be run on high-performance computing (HPC) platforms because users do not have administrative privileges._

[Apptainer](https://apptainer.org/) (formerly Singularity) is a container engine that does not require administrative privileges to execute and was developed specifically for HPC environments. Therefore, it is safe to run on CCR's HPC platforms. Docker images are widely available for many software packages, therefore a common use case is to use Apptainer to run Docker images.  Users can also build Apptainer containers to run on CCR's clusters.  

## Apptainer

Apptainer is a containerization software package that does not require users to have administrative privileges when running containers, and can thus be safely used on Research Computing resources. Much like Docker, Apptainer is a containerization software designed around compartmentalization of applications, libraries, and workflows. This is done through the creation of compressed images in the `.sif` format which can be run as ephemeral containers. Unlike Docker, however, Apptainer does not manage images, containers, or volumes through a central application. Instead, Apptainer generates saved image files that can either be mutable or immutable based on compression.

### Setting up Temporary Storage Directories  

Images generated by Apptainer can be _large_. It is common for containers to exceed 10GB. By default, Apptainer stores cache files in `/user/[YourCCRusername]/.apptainer/cache` This means you can easily exceed your 25GB home directory quota while pulling a couple of containers! In addition to quota problems, you may also see error messages when pulling containers from remote repositories mentioning how `/tmp` does not have write permissions. These problems and more can be avoided by setting an environment variable before pulling containers.  Below are several examples of how to do so. The first demonstrates how to set the Apptainer cache directory to a subdirectory in your group's shared project directory.  NOTE: this may not be in `/projects/academic` as shown in this example and you'll need to make sure this subdirectory is created before running Apptainer:

```
export APPTAINER_CACHEDIR=/projects/academic/[YourGroupName]/[CCRusername]/cache
```

Alternatively, you can set the cache directory to your job's Slurm temporary directory which gets set to `/scratch/$JOBID` and is automatically deleted when your job ends.  This may result in faster container downloads but if your group is using the same container for multiple builds, you'll want to use your shared project directory for the `APPTAINER_CACHE` location as shown above.  

```
export APPTAINER_CACHEDIR=$SLURMTMPDIR  
```

### Pulling Images

Pulling images from public repositories is often the easiest method of using a containerized application. Be aware large containers can take a long time to download.  We recommend you pull containers on a compute node in a [running job](../hpc/jobs.md#interactive-job-submission).  The compile nodes can be used as well; however, very large containers may not successfully build.  Apptainer is not available on the CCR login nodes.  See more on node types [here](../hpc/clusters.md).

We can use the `apptainer pull` command to remotely download our chosen image file and convert it to the Apptainer `.sif` format. The command requires the container registry we would like to use, followed by the repository’s name:

```
apptainer pull <localname>.sif <container-registry>://<repository-name>
```

Where `<localname>.sif` is the name you choose for the Apptainer image. 

A container registry is simply a server that manages uploaded containers. Docker Hub is the most widely used register. To pull a container image from Docker Hub:

```
apptainer pull docker://another:example
```

### Running a SIF image as a container

SIF images can be run as containers much like Docker images. Apptainer commands, however, follow a bit more nuanced syntax depending on what you’d like to do. After pulling your image from Docker Hub you can run the image by using the `apptainer run` command. Type:

```
apptainer run <image-name>
```

Running a container will execute the default program that the container developer will have specified in container definition file. To execute specific programs in your container, we can use the `apptainer exec` command, and then specify the program:

```
apptainer exec <image-name> <program>
```

Much like specifying an application in Docker, this will allow a user to execute any program that is installed within your container. Unlike Docker however, you do not need to specify a shell application to shell into the container. We can simply use the `apptainer shell` command:

```
apptainer shell <image-name>
```

*Example:*

Say we have an image that contains python 3.7 as the default software, and we want to run python from the container. We can do this with the command:

```
apptainer run python-cont.sif
```

If the default application for the image is not python we could run python as follows:

```
apptainer exec python-cont.sif python
```

### File Access

By default, when using Apptainer, only `/user/$USER` (your home directory), is available within any given container. This means that a user will need to bind any other required folders to the container’s directory tree. Furthermore, a container will also have access to the files in the same folder where it was initialized (`$PWD`). 

To bind any additional folders or files to your container, you can utilize the `-B` flag in your Apptainer run, exec, and shell commands:

```
apptainer run -B /source/directory:/target/directory sample-image.sif
```
For example, if you wanted to have access to your group's project directory within your container, you would use the bind option like this and you'd find your project directory in the container in `/projects`:  

```
apptainer run -B /projects/academic/[YourGroupName]:/projects sample-image.sif
```

Alternatively, you can bind directories by utilizing the `APPTAINER_BINDPATH` environment variable. Simply export a list of directory pairs you would like to bind inside your container:

```
export APPTAINER_BINDPATH=/source/directory1:/target/directory1,\
/source/directory2:/target/directory2
```

Then run, execute, or shell into the container as normal.

### Building Images with Apptainer

!!! Danger "Compute Node Use Only"  
    We recommend you build and pull containers on a compute node in a [running job](../hpc/jobs.md#interactive-job-submission).  Apptainer is not available on the CCR login nodes and some features may  not work on the compile nodes.  See more on node types [here](../hpc/clusters.md) 

In the event that a container is unavailable for a given application, you may need to build your own container from scratch. Apptainer allows a user to build images using a *definition file*. Just like a Dockerfile, this file has a variety of directives that allow for the customization of your image. A sample image would look something like this: 

```
Bootstrap: docker
From: ubuntu:24.04

%help
    I am help text!

%post	
    apt-get update
    apt-get install nano
    apt-get install gcc 

%runscript
    echo “hello! I am a container!”
```

Once you have written your Apptainer definition file, you can build the application with the `apptainer build` command, as follows:

```
apptainer build <localname>.sif <recipe-name>.def
```
!!! Tip "Did your build fail?"  
    Not all containers will build on our systems.  If you run into an error, you may need to install Apptainer on your local machine and create your container there.  Then upload it to CCR's systems. Instructions to do this can be found in the [documentation for Apptainer](https://apptainer.org/docs/admin/main/installation.html). 

Now, run the container you just built:  

```
apptainer run test.sif
“hello! I am a container!”
```


### Building MPI-enabled images
MPI-enabled Apptainer containers can be deployed on CCR's systems with the caveat that the MPI software within the container may have a similar (not necessarily exact) version of the MPI software available on the system. This requirement diminishes the portability of MPI-enabled containers, as they may not run on other systems without compatible MPI software. Regardless, MPI-enabled containers can still be a very useful option in many cases. 

Here we provide an example of using a gcc compiler with OpenMPI. CCR's system uses an Infiniband interconnect. In order to use a Apptainer container with OpenMPI (or any MPI) on the cluster, OpenMPI needs to be installed both inside and outside of the container. More specifically, the _same_ version of OpenMPI needs to be installed inside and outside (at least very similar, you can sometimes get away with two different minor versions, e.g. 2.1 and 2.0). 


Once you’ve built the container with one of the methods outlined above, you can place it in your home or project directory and run it on a compute node. The following is an example of running a gcc/OpenMPI container with Apptainer. The syntax is a normal MPI run where multiple instances of an Apptainer image are run. The following example runs `mpi_hello_world` with MPI from a container.

```
module load gcc/11.2.0
module load openmpi/4.1.1

mpirun -np 4 apptainer exec openmpi.sif mpi_hello_world
```

### GPU-enabled Containers with Apptainer

It is possible to run GPU workloads within Apptainer containers. To do so, merely add the `--nv` flag when you use `run` or `exec` commands like so:

`apptainer run --nv <image_name>.sif`

NVIDIA also hosts a [number of containers](https://catalog.ngc.nvidia.com/containers) as part of their own container library. This will allow you to run more up-to-date versions of software like PyTorch which may have older versions installed in CCR's [software environment](../software/modules.md).  These can be pulled and run by Apptainer, though the pulling process can take several hours depending on the image size.  As stated previously, we recommend you do this on a compute node in a [running job](../hpc/jobs.md#interactive-job-submission).  

NOTE: The URL format for using the NVIDIA container registry with Apptainer is:  
`docker://nvcr.io/nvidia/name:version`

#### Example GPU container workflow  

The Python-based software applications that we get the most requests for (i.e. Pytorch and Torch Lightning) are updated frequently and aren't particularly easy to install with Easybuild.  For this reason, we recommend utilizing the NVIDIA containers that are available for free, are updated frequently, and work on CCR's systems.  NVIDIA provides a [Framework Containers Support Matrix](https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html) which has information on what software versions are included in the containers as well as what type of prerequisites the containers may have.  The deep learning framework container packages follow a naming convention that is based on the year and month of the image release.  We recommend using the 24.xx and 23.xx versions but the 22.xx containers should also work; they just have older versions of Ubuntu and Python.  

**Tips on how to navigate the NVIDIA catalog:**  

- Looking at the [Pytorch info](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch), we see a description of the package on the main page that includes instructions on how to run the container, what is in the container, and more on the Pytorch software itself (NOTE: container run commands are in Docker format which is slightly different than Apptainer)  
- On the left is the latest version name (tag) and when it was published.  This tag name is version that you'll input in the pull, run, and exec commands    
- If you want a different version than the latest, click on the `Tags` tab at the top of the page  
- Make sure to select a version that supports AMD64 architecture (not the containers named `-igpu`)  

CCR currently provides Pytorch 1.13.1 as a software module, which is from October 2022.  There have been many releases since then and much interest in Pytorch 2.  For this example, we will pull a container with Pytorch 2.6 installed.   

1: Login to CCR's HPC environment  
2: Request resources to run an [interactive job](../hpc/jobs.md#interactive-job-submission). These containers can be quite large and this process can take quite a long time. Make sure to request a job walltime of at least a few hours.  

3: Once on the compute node, you'll run the following to set your cache directory and pull a container from the NVIDIA library:  

```
export APPTAINER_CACHEDIR=/projects/academic/[YourGroupName]/[CCRusername]/cache 
cd /projects/academic/[YourGroupName]/[CCRusername]/container-directory  #This should be whatever directory you want to store your container in
apptainer pull pytorch26.sif docker://nvcr.io/nvidia/pytorch:24.12-py3 
```

4: When this completes you should see a file in your directory named `pytorch26.sif`  Let's shell into the container.  Once in the container, check out the version of python you've got, load pytorch and check the version:  

```
apptainer shell --nv pytorch26.sif  
Apptainer> which python
/usr/bin/python
Apptainer> python --version
Python 3.12.3
Apptainer> python
Python 3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import torch
>>> print(torch.__version__)
2.6.0a0+df5bbc09d1.nv24.12
>>> exit()
Apptainer> exit
exit
```

5: Now you've got an updated container, running a much newer version of pytorch than what CCR provides!  That's great but what do you do if you want to install additional software?  We recommend using a python virtual environment.  You can store this virtual environment in your group's project space so that it is outside the container and backed up.  If you're not familiar with virtual environments, check out our [documentation here](../howto/python.md#virtual-environments). For this example, we'll install a popular python package `imageio` in a virtual environment and access it in the container. This time when we start our container, we're going to bind mount our group's project directory so we can access it in the container.  

```
apptainer shell --nv -B /projects/academic/[YourGroupName]:/projects pytorch26.sif  
Apptainer> cd /projects
Apptainer> python3 -mvenv --system-site-packages myenv 
Apptainer> source myenv/bin/activate
(myenv) Apptainer> pip install imageio
Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
Collecting imageio
  Downloading imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)
Collecting numpy (from imageio)
  Downloading numpy-2.2.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.0/62.0 kB 2.8 MB/s eta 0:00:00
Collecting pillow>=8.3.2 (from imageio)
  Downloading pillow-11.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.1 kB)
Downloading imageio-2.37.0-py3-none-any.whl (315 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 315.8/315.8 kB 7.1 MB/s eta 0:00:00
Downloading pillow-11.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.5/4.5 MB 105.6 MB/s eta 0:00:00
Downloading numpy-2.2.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.1/16.1 MB 248.0 MB/s eta 0:00:00
Installing collected packages: pillow, numpy, imageio
Successfully installed imageio-2.37.0 numpy-2.2.2 pillow-11.1.0
(myenv) Apptainer> python
Python 3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import imageio
>>> print(imageio.__version__)
2.37.0
>>> exit()
Apptainer> exit
```

6: Test this out using a GPU node:  

```
apptainer shell --nv -B /projects/academic/[YourGroupName]:/projects pytorch26.sif
Apptainer> source /projects/myenv/bin/activate
(myenv) Apptainer> python
Python 3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import torch
>>> import imageio
>>> print(torch.__version__)
2.6.0a0+df5bbc09d1.nv24.12
>>> print(imageio.__version__)
2.37.0
>>> num_of_gpus = torch.cuda.device_count()
>>> print(num_of_gpus)
1
>>> exit()
```

**IMPORTANT NOTES:**  
This virtual environment is accessible outside of the container as well as inside, but some key things to keep in mind:  

- Make sure to create the virtual environment IN the container.  
- The virtual environment includes system site packages which will only work inside the container.  So although you can access it outside of the container, it can really only be used inside.    
- Make sure to specify the full path of the virtual environment python executable in any scripts you run otherwise it will automatically use your container's python.  In this example that would be:  `/projects/myenv/bin/python3` in the container.  The full path to that is `/projects/academic/[YourGroupName]/myenv/bin/python3` outside the container.  
- Not all packages will install correctly inside a virtual environment.  See our warnings over [here](../howto/python.md#installing-packages-inside-the-virtual-environment).  
- Because you are intentially attempting to use already installed python packages with your virtual environment, you may run into conflicts.  In that event, you may need to [create your own container](#building-images-with-apptainer) from scratch installing all the packages you need rather than using a pre-built one from NVIDIA.  
- If you're using a container, we do NOT recommend also using modules CCR's [software environment](../software/modules.md).  These will most likely conflict.
